## 1. 声音识别的原理（和视觉对比）
图像：是二维信号，输入卷积网络提取空间纹理特征。
音频：是一维随时间变化的波形，本质上就是空气震动的频率和能量随时间的变化。

人耳识别声音靠的是频率模式，不同狗的声音在声道、声带结构上有差异，因此它们的叫声频率分布和时序模式不同。
所以传统音频识别就是：
波形 → 频谱（傅里叶变换） → 特征提取（比如 MFCC） → 匹配/分类。

## 2. 什么是 MFCC（Mel-Frequency Cepstral Coefficients）
MFCC 是语音识别里最经典的特征，基本上属于“语音版的卷积特征”。
流程：
分帧（比如每 25ms 为一帧，步长 10ms） → 保证局部平稳。
FFT（快速傅里叶变换） → 得到功率谱。
Mel滤波器组 → 把频率轴压缩到符合人耳听感的尺度（低频更精细，高频更粗略）。
取对数（log） → 变换成更接近人耳的 loudness 感知。
离散余弦变换 (DCT) → 把频率能量模式压缩成一组系数（一般取前 13 或 40 维）。
你可以理解为：
MFCC 就是把狗叫声转成一组稳定的、紧凑的频率特征向量，类似于图像的 SIFT/HOG。

## 3、如何提升鲁棒性
MFCC 在安静环境下很好，但对噪声敏感。更鲁棒的方案有：
1、PLP (Perceptual Linear Prediction)：和 MFCC 类似，但更贴近人耳听感建模
2、Log-Mel Filterbank Energies (FBank)：比 MFCC 少一步 DCT，保留更多原始频谱形态（更鲁棒，现代深度学习都用它）。
3、RASTA-PLP：在 PLP 上加了滤波，可以去除慢变化噪声。
4、Spectral Features 组合：比如谱质心、谱带宽、谱平坦度、零交叉率，这些对环境干扰没那么敏感。
5、特征增强/归一化： CMVN（Cepstral Mean Variance Normalization，去除通道效应） 。加强前处理：降噪、带通滤波、VAD（去掉静音）。
6、统计特征：对整段音频的 MFCC 求 mean/std/skew/kurtosis → 就像做 embedding pooling，能抵抗瞬时抖动。
实际方案需要实验得出, 目前倾向于：Log-Mel + Δ/ΔΔ + 统计量 + 简单归一化。

## 4、部署在 MCU 上的可能性
1、MCU资源: MCU 典型资源：内存几十 KB ~ 数 MB，主频几十 MHz ~ 数百 MHz。 没有 GPU，没有 FPU 。所以深度学习几乎跑不动（即便 tiny model 也要用量化 + 特殊优化）。

2、传统方法优势: 
MFCC 提取：可以用整数 FFT（很多 DSP 库都有，比如 CMSIS-DSP）。
模板匹配：cosine / euclidean 距离计算非常轻量（几百维的向量 dot product）。
VAD：能量阈值法，简单又快。
这样整个 pipeline 在 Cortex-M4/M7 上完全可实时跑。

3、实际部署方式:
在 PC 上开发 → 导出配置好的参数（MFCC 维度、模板 embedding）。
在 MCU 上：
用 CMSIS-DSP 或 KissFFT 算 MFCC/Log-Mel。
存每只狗的模板向量。
新音频 → 提特征 → 求相似度 → 输出结果。
如果 MCU 太弱，可以用专用语音芯片（比如 ESP32-S3 + I2S 麦克风），跑实时识别是可行的。

4、和深度学习的对比
深度学习：更强，更鲁棒（尤其能区分相似个体），但需要更多数据、算力、部署难度高。
传统特征 + 模板匹配：小数据下效果很好（尤其是你只区分自家 3~5 条狗），能在 MCU 上运行，开发和维护成本低。
所以选择“传统方法先做”是非常明智的，尤其是在端侧。




